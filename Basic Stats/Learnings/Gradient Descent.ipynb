{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b11e66-e542-416c-ab45-e967c46388b7",
   "metadata": {},
   "source": [
    "# Learning Gradient Descent.\n",
    "\n",
    "Playing around with gradient descent. Learning about loss functions. \n",
    "\n",
    "Loss function is given by L(y, t). Where y is the output of the model, and t is the target of the actual data being predicted.\n",
    "\n",
    "The goal of gradient descent is to converge the difference between y and t to 0. This means a higher loss is worse, and a lower loss is better. It indicates the delta of y and t is growing smaller.\n",
    "\n",
    "Each tick of the learning rate adjusts parameters used to generate y, which is checked against t. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dcf05-cfa2-4677-923c-b95c15d76817",
   "metadata": {},
   "source": [
    "The loss L(y, t) function measures how well the model's predictions y match the actual target values t. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "Gradient Descent: This optimization algorithm iteratively adjusts the model parameters to minimize the loss function. The goal is to reduce the difference between the predicted output y and the actual target t, thereby minimizing the loss.\n",
    "\n",
    "Learning Rate: This hyperparameter controls the size of the steps taken during gradient descent. Each iteration updates the model parameters in the direction that reduces the loss the most, with the step size determined by the learning rate.\n",
    "\n",
    "Convergence: The process of gradient descent continues until the loss function converges to a minimum value, ideally as close to zero as possible, indicating that the model's predictions are very close to the actual targets.\n",
    "\n",
    "This iterative process continues until the loss function reaches a minimum value, indicating that the model has learned the best possible parameters to predict the target values accurately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b2891-03c7-4d5c-a894-09cd6e82b157",
   "metadata": {},
   "source": [
    "Here's a step-by-step process: \n",
    "1. Forward Pass: Compute the predicted value y using the current weights and biases.\n",
    "2. Compute Loss: Calculate the loss L(y, t) using the predicted value y and the actual value t.\n",
    "3. Backward Pass: Compute the gradients of the loss with respect to the weights and biases. Basically, the vector of partial derivatives of the loss function for each parameter. The gradient indicates the direciton and rate of the steepest increase of the loss function. \n",
    "4. Update Parameters: Adjust the weights and biases using the gradients and the learning rate. The partial derivatives are used to update the model parameters in the opposite direction of the gradient to minimise the loss function. The gradient points in the direction of the steepest ascent, so moving in the opposite direction (i.e., subtracting the gradient) helps in descending the loss function towards a local minimum.\n",
    "\n",
    "In summary, t is the actual value, y is the predicted value, and the loss function L(y,t) measures how far y is from t. The goal of gradient descent is to adjust the weights and biases to make y as close to t as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15473ea-fe39-4d21-a823-f41fce28fab8",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. The main goal is to find the set of model parameters (weights and biases) that result in the smallest possible loss. There are several variations of gradient descent, including:\n",
    "\n",
    "Batch Gradient Descent: This version calculates the gradient using the entire training dataset. While it provides a stable and accurate estimate of the gradient, it can be computationally expensive for large datasets.\n",
    "\n",
    "Stochastic Gradient Descent (SGD): Instead of using the entire dataset, SGD updates the model parameters using the gradient of the loss function for a single training example at a time. This makes the updates noisier but allows the algorithm to potentially escape local minima and find a better global minimum.\n",
    "\n",
    "Mini-Batch Gradient Descent: This is a compromise between batch gradient descent and SGD. It calculates the gradient using a small batch of training examples. This approach is commonly used as it balances the efficiency of batch gradient descent and the noise reduction of SGD.\n",
    "\n",
    "The learning rate \n",
    "Î· is a crucial hyperparameter that determines the size of the steps taken during gradient descent. If the learning rate is too high, the algorithm might overshoot the minimum and fail to converge. If the learning rate is too low, the algorithm will converge very slowly and may get stuck in a local minimum.\n",
    "\n",
    "Convergence\n",
    "Convergence is achieved when the changes in the loss function become very small, indicating that the algorithm has reached a minimum. Monitoring the loss function over iterations helps in determining when to stop the training process.\n",
    "\n",
    "Challenges and Solutions\n",
    "Local Minima: The loss function may have multiple local minima. Gradient descent can get stuck in a local minimum instead of finding the global minimum. Techniques like adding noise (SGD) or using advanced optimization algorithms (e.g., Adam, RMSprop) can help mitigate this issue.\n",
    "\n",
    "Vanishing and Exploding Gradients: In deep neural networks, gradients can become very small (vanishing) or very large (exploding). Techniques like gradient clipping, proper weight initialization, and using activation functions like ReLU can help address these problems.\n",
    "\n",
    "Choosing the Right Learning Rate: Using learning rate schedules (e.g., reducing the learning rate as training progresses) or adaptive learning rates (e.g., AdaGrad, Adam) can help optimize the learning rate dynamically.\n",
    "\n",
    "Advanced Optimization Algorithms\n",
    "In addition to basic gradient descent, several advanced optimization algorithms are commonly used:\n",
    "\n",
    "- Momentum: This technique helps accelerate gradient descent by adding a fraction of the previous update to the current update.\n",
    "- AdaGrad: Adjusts the learning rate for each parameter based on the historical gradients, allowing for larger updates for infrequent parameters.\n",
    "- RMSprop: Similar to AdaGrad but uses a moving average of squared gradients to adjust the learning rate, preventing the learning rate from becoming too small.\n",
    "- Adam: Combines the benefits of Momentum and RMSprop, using both first-order (mean) and second-order (variance) moments of the gradients to adapt the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c7f1b5-36d5-4790-9acb-98532b33cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800fea8-03ab-4664-8df2-a3221c29ba5c",
   "metadata": {},
   "source": [
    "Generate Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762d9dee-5da6-4d09-aa5d-eed0ee4b6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = 100000\n",
    "\n",
    "xs = np.random.uniform(low =-100, high = 100, size = (obs,1))\n",
    "zs = np.random.uniform(low =-100, high = 100, size = (obs,1))\n",
    "\n",
    "inputs = np.column_stack((xs,zs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c19a4-f074-4ac1-8f34-2034c588e349",
   "metadata": {},
   "source": [
    "Fake Targets to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce890b8-abcf-4630-a1eb-15a3f5c2b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = np.random.uniform(low=-100, high=100, size=(obs, 1))\n",
    "\n",
    "targets = 15*xs - 13*zs + 6 + error #xs, zs are the weights. 6 is bias. Error is just the noise in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d690180-a5ec-4558-977a-5d0f5124d528",
   "metadata": {},
   "source": [
    "Make the Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd81e74-d096-4eb3-9604-e99192da0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_range = 0.002\n",
    "\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "bias = np.random.uniform(low=-init_range, high=init_range, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327971d-7c84-4559-91c5-31c30e60ce08",
   "metadata": {},
   "source": [
    "Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37dd6151-e282-4ccc-a8e9-9d547e567bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299d10a-e9cd-437c-ba53-916bf962fccc",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba777baf-d2d6-4aa5-b728-e8313acec5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 131509451709.95451\n",
      "Iteration 2: Loss = 564628005536449.1\n",
      "Iteration 3: Loss = 2.430431297308273e+18\n",
      "Iteration 4: Loss = 1.0461775042511154e+22\n",
      "Iteration 5: Loss = 4.503272624156164e+25\n",
      "Iteration 6: Loss = 1.9384380891007085e+29\n",
      "Iteration 7: Loss = 8.344041623906968e+32\n",
      "Iteration 8: Loss = 3.5917145961130486e+36\n",
      "Iteration 9: Loss = 1.5460659186990108e+40\n",
      "Iteration 10: Loss = 6.655106690634975e+43\n",
      "Iteration 11: Loss = 2.8647245168150722e+47\n",
      "Iteration 12: Loss = 1.233137580973406e+51\n",
      "Iteration 13: Loss = 5.308123600428068e+54\n",
      "Iteration 14: Loss = 2.2849218148720512e+58\n",
      "Iteration 15: Loss = 9.835636810118586e+61\n",
      "Iteration 16: Loss = 4.23384002825609e+65\n",
      "Iteration 17: Loss = 1.822498693547886e+69\n",
      "Iteration 18: Loss = 7.845142304507515e+72\n",
      "Iteration 19: Loss = 3.37703252642204e+76\n",
      "Iteration 20: Loss = 1.453685565712267e+80\n",
      "Iteration 21: Loss = 6.257581694367192e+83\n",
      "Iteration 22: Loss = 2.693663815877197e+87\n",
      "Iteration 23: Loss = 1.1595275446765615e+91\n",
      "Iteration 24: Loss = 4.9913676403653064e+94\n",
      "Iteration 25: Loss = 2.1486162657378737e+98\n",
      "Iteration 26: Loss = 9.249089219027702e+101\n",
      "Iteration 27: Loss = 3.981437354705331e+105\n",
      "Iteration 28: Loss = 1.7138847400972613e+109\n",
      "Iteration 29: Loss = 7.377753540215568e+112\n",
      "Iteration 30: Loss = 3.175904825876262e+116\n",
      "Iteration 31: Loss = 1.367135683272085e+120\n",
      "Iteration 32: Loss = 5.885136779616109e+123\n",
      "Iteration 33: Loss = 2.5333914722785574e+127\n",
      "Iteration 34: Loss = 1.0905582003596368e+131\n",
      "Iteration 35: Loss = 4.694574009146606e+134\n",
      "Iteration 36: Loss = 2.020897701692248e+138\n",
      "Iteration 37: Loss = 8.699479447303216e+141\n",
      "Iteration 38: Loss = 3.7449239106871343e+145\n",
      "Iteration 39: Loss = 1.612105738593572e+149\n",
      "Iteration 40: Loss = 6.939766251943847e+152\n",
      "Iteration 41: Loss = 2.9874246007050752e+156\n",
      "Iteration 42: Loss = 1.2860263105756812e+160\n",
      "Iteration 43: Loss = 5.536095126259011e+163\n",
      "Iteration 44: Loss = 2.383186456602546e+167\n",
      "Iteration 45: Loss = 1.0259195060612877e+171\n",
      "Iteration 46: Loss = 4.416409526267523e+174\n",
      "Iteration 47: Loss = 1.9011928537848026e+178\n",
      "Iteration 48: Loss = 8.184342427117606e+181\n",
      "Iteration 49: Loss = 3.523239709746289e+185\n",
      "Iteration 50: Loss = 1.5167058906884154e+189\n",
      "Iteration 51: Loss = 6.529220484384045e+192\n",
      "Iteration 52: Loss = 2.810749058567053e+196\n",
      "Iteration 53: Loss = 1.2099950411186863e+200\n",
      "Iteration 54: Loss = 5.2088983578378404e+203\n",
      "Iteration 55: Loss = 2.2423786531899517e+207\n",
      "Iteration 56: Loss = 9.653233054625333e+210\n",
      "Iteration 57: Loss = 4.1556350884022226e+214\n",
      "Iteration 58: Loss = 1.7889688929685294e+218\n",
      "Iteration 59: Loss = 7.701386925463706e+221\n",
      "Iteration 60: Loss = 3.315399490144328e+225\n",
      "Iteration 61: Loss = 1.4272615192971334e+229\n",
      "Iteration 62: Loss = 6.144294359377311e+232\n",
      "Iteration 63: Loss = 2.645094761374078e+236\n",
      "Iteration 64: Loss = 1.13870498264187e+240\n",
      "Iteration 65: Loss = 4.902097774613067e+243\n",
      "Iteration 66: Loss = 2.1103450926169544e+247\n",
      "Iteration 67: Loss = 9.08501708823136e+250\n",
      "Iteration 68: Loss = 3.9110987115916593e+254\n",
      "Iteration 69: Loss = 1.683730440024711e+258\n",
      "Iteration 70: Loss = 7.248482694479366e+261\n",
      "Iteration 71: Loss = 3.120487192777432e+265\n",
      "Iteration 72: Loss = 1.343378695508776e+269\n",
      "Iteration 73: Loss = 5.783293877650536e+272\n",
      "Iteration 74: Loss = 2.4897332426199606e+276\n",
      "Iteration 75: Loss = 1.0718428736892238e+280\n",
      "Iteration 76: Loss = 4.61434625950756e+283\n",
      "Iteration 77: Loss = 1.9865064732844204e+287\n",
      "Iteration 78: Loss = 8.552055981366185e+290\n",
      "Iteration 79: Loss = 3.681729107146244e+294\n",
      "Iteration 80: Loss = 1.5850168048024412e+298\n",
      "Iteration 81: Loss = 6.823648607041251e+301\n",
      "Iteration 82: Loss = 2.9376508624235645e+305\n",
      "Iteration 83: Loss = inf\n",
      "Iteration 84: Loss = inf\n",
      "Iteration 85: Loss = inf\n",
      "Iteration 86: Loss = inf\n",
      "Iteration 87: Loss = inf\n",
      "Iteration 88: Loss = inf\n",
      "Iteration 89: Loss = inf\n",
      "Iteration 90: Loss = inf\n",
      "Iteration 91: Loss = inf\n",
      "Iteration 92: Loss = inf\n",
      "Iteration 93: Loss = inf\n",
      "Iteration 94: Loss = inf\n",
      "Iteration 95: Loss = inf\n",
      "Iteration 96: Loss = inf\n",
      "Iteration 97: Loss = inf\n",
      "Iteration 98: Loss = inf\n",
      "Iteration 99: Loss = inf\n",
      "Iteration 100: Loss = inf\n",
      "Final weights: [[-7.03154048e+182]\n",
      " [ 6.96933038e+182]]\n",
      "Final bias: [-1.17689651e+179]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Conda\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "C:\\Users\\stefe\\AppData\\Local\\Temp\\ipykernel_12480\\3615837643.py:5: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.sum(delta ** 2)\n"
     ]
    }
   ],
   "source": [
    "for i in range (100):\n",
    "    outputs = np.dot(inputs,weights) + bias\n",
    "    delta = outputs - targets\n",
    "    \n",
    "    loss = np.sum(delta ** 2)\n",
    "\n",
    "    print(f\"Iteration {i+1}: Loss = {loss}\")\n",
    "\n",
    "    delta_scaled = delta / obs\n",
    "\n",
    "    weights = weights - learning_rate * np.dot(inputs.T,delta_scaled)\n",
    "    bias = bias - learning_rate * np.sum(delta_scaled)\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ef1e9-6315-4f14-93e7-01f37319626c",
   "metadata": {},
   "source": [
    "### Here's an example of the exploding gradient problem. \n",
    "This occurs when the gradients during training become excessively large. This typically happens in deep neural networks during backpropagation when the gradients are propagated backward through many layers, resulting in very large updates to the weights. Consequently, the model parameters can become so large that they cause numerical instability, often leading to the loss function becoming very large and eventually reaching infinity (or inf in practical terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aacd03c-b26e-4738-9a7e-c484822b6d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 131500265099.56976\n",
      "Iteration 2: Loss = 123755192183.14716\n",
      "Iteration 3: Loss = 116467459944.08029\n",
      "Iteration 4: Loss = 109610062750.02641\n",
      "Iteration 5: Loss = 103157589632.19342\n",
      "Iteration 6: Loss = 97086130121.53886\n",
      "Iteration 7: Loss = 91373185645.28008\n",
      "Iteration 8: Loss = 85997586155.38225\n",
      "Iteration 9: Loss = 80939411680.08014\n",
      "Iteration 10: Loss = 76179918507.7304\n",
      "Iteration 11: Loss = 71701469729.45924\n",
      "Iteration 12: Loss = 67487469883.22073\n",
      "Iteration 13: Loss = 63522303457.08015\n",
      "Iteration 14: Loss = 59791277023.837845\n",
      "Iteration 15: Loss = 56280564792.56492\n",
      "Iteration 16: Loss = 52977157375.28454\n",
      "Iteration 17: Loss = 49868813578.94631\n",
      "Iteration 18: Loss = 46944015044.0526\n",
      "Iteration 19: Loss = 44191923561.843\n",
      "Iteration 20: Loss = 41602340911.87086\n",
      "Iteration 21: Loss = 39165671071.14337\n",
      "Iteration 22: Loss = 36872884654.78662\n",
      "Iteration 23: Loss = 34715485456.4651\n",
      "Iteration 24: Loss = 32685478964.566803\n",
      "Iteration 25: Loss = 30775342737.48603\n",
      "Iteration 26: Loss = 28977998528.22558\n",
      "Iteration 27: Loss = 27286786055.022034\n",
      "Iteration 28: Loss = 25695438320.797577\n",
      "Iteration 29: Loss = 24198058389.981316\n",
      "Iteration 30: Loss = 22789097536.643116\n",
      "Iteration 31: Loss = 21463334682.965122\n",
      "Iteration 32: Loss = 20215857051.857292\n",
      "Iteration 33: Loss = 19042041962.02242\n",
      "Iteration 34: Loss = 17937539698.00999\n",
      "Iteration 35: Loss = 16898257391.781364\n",
      "Iteration 36: Loss = 15920343856.057222\n",
      "Iteration 37: Loss = 15000175313.245226\n",
      "Iteration 38: Loss = 14134341967.06445\n",
      "Iteration 39: Loss = 13319635367.105915\n",
      "Iteration 40: Loss = 12553036519.50693\n",
      "Iteration 41: Loss = 11831704699.6816\n",
      "Iteration 42: Loss = 11152966925.651783\n",
      "Iteration 43: Loss = 10514308052.970226\n",
      "Iteration 44: Loss = 9913361454.531517\n",
      "Iteration 45: Loss = 9347900250.733479\n",
      "Iteration 46: Loss = 8815829057.491285\n",
      "Iteration 47: Loss = 8315176221.525376\n",
      "Iteration 48: Loss = 7844086514.150062\n",
      "Iteration 49: Loss = 7400814256.488606\n",
      "Iteration 50: Loss = 6983716850.639417\n",
      "Iteration 51: Loss = 6591248692.822133\n",
      "Iteration 52: Loss = 6221955445.948056\n",
      "Iteration 53: Loss = 5874468650.391117\n",
      "Iteration 54: Loss = 5547500652.988849\n",
      "Iteration 55: Loss = 5239839835.482174\n",
      "Iteration 56: Loss = 4950346124.712307\n",
      "Iteration 57: Loss = 4677946767.937235\n",
      "Iteration 58: Loss = 4421632357.612626\n",
      "Iteration 59: Loss = 4180453090.9065275\n",
      "Iteration 60: Loss = 3953515250.0869074\n",
      "Iteration 61: Loss = 3739977890.7397275\n",
      "Iteration 62: Loss = 3539049725.5452724\n",
      "Iteration 63: Loss = 3349986192.0651526\n",
      "Iteration 64: Loss = 3172086693.67431\n",
      "Iteration 65: Loss = 3004692003.413904\n",
      "Iteration 66: Loss = 2847181821.144753\n",
      "Iteration 67: Loss = 2698972474.9490137\n",
      "Iteration 68: Loss = 2559514758.2623725\n",
      "Iteration 69: Loss = 2428291894.721938\n",
      "Iteration 70: Loss = 2304817623.188323\n",
      "Iteration 71: Loss = 2188634395.8457265\n",
      "Iteration 72: Loss = 2079311682.7028277\n",
      "Iteration 73: Loss = 1976444376.2116127\n",
      "Iteration 74: Loss = 1879651290.0922472\n",
      "Iteration 75: Loss = 1788573746.801182\n",
      "Iteration 76: Loss = 1702874248.4081857\n",
      "Iteration 77: Loss = 1622235225.9570677\n",
      "Iteration 78: Loss = 1546357862.6756873\n",
      "Iteration 79: Loss = 1474960986.6745014\n",
      "Iteration 80: Loss = 1407780029.0304072\n",
      "Iteration 81: Loss = 1344566043.3949234\n",
      "Iteration 82: Loss = 1285084783.4937499\n",
      "Iteration 83: Loss = 1229115835.0992558\n",
      "Iteration 84: Loss = 1176451799.2593186\n",
      "Iteration 85: Loss = 1126897523.7558568\n",
      "Iteration 86: Loss = 1080269379.9451365\n",
      "Iteration 87: Loss = 1036394582.3000906\n",
      "Iteration 88: Loss = 995110548.1331248\n",
      "Iteration 89: Loss = 956264295.1267884\n",
      "Iteration 90: Loss = 919711874.439784\n",
      "Iteration 91: Loss = 885317837.2876148\n",
      "Iteration 92: Loss = 852954733.0212257\n",
      "Iteration 93: Loss = 822502636.8436953\n",
      "Iteration 94: Loss = 793848705.4148873\n",
      "Iteration 95: Loss = 766886758.6972849\n",
      "Iteration 96: Loss = 741516886.4934992\n",
      "Iteration 97: Loss = 717645078.2174197\n",
      "Iteration 98: Loss = 695182874.5270841\n",
      "Iteration 99: Loss = 674047039.5283362\n",
      "Iteration 100: Loss = 654159252.3346075\n",
      "Final weights: [[ 14.28190345]\n",
      " [-12.37987878]]\n",
      "Final bias: [0.00778052]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_range = 0.002\n",
    "\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "bias = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "\n",
    "learning_rate = 0.000009\n",
    "\n",
    "for i in range (100):\n",
    "    outputs = np.dot(inputs,weights) + bias\n",
    "    delta = outputs - targets\n",
    "    \n",
    "    loss = np.sum(delta ** 2)\n",
    "\n",
    "    print(f\"Iteration {i+1}: Loss = {loss}\")\n",
    "\n",
    "    delta_scaled = delta / obs\n",
    "\n",
    "    weights = weights - learning_rate * np.dot(inputs.T,delta_scaled)\n",
    "    bias = bias - learning_rate * np.sum(delta_scaled)\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6b94b-0f1b-4b0b-a5e6-ac8c920704f7",
   "metadata": {},
   "source": [
    "### Here's an example of the vanishing gradient problem\n",
    "The vanishing gradient problem occurs when the gradients of the loss function with respect to the model parameters become very small during training. This leads to extremely small updates to the model parameters, effectively causing the training to slow down or stop. This is particularly problematic in deep neural networks, where the gradients can diminish as they are propagated back through many layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4b9ff19-b131-4939-9af5-95cb253d990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 132123550221.1919\n",
      "Iteration 2: Loss = 56185205358971.375\n",
      "Iteration 3: Loss = 2.3953539586338244e+16\n",
      "Iteration 4: Loss = 1.0212237440193829e+19\n",
      "Iteration 5: Loss = 4.353845371030912e+21\n",
      "Iteration 6: Loss = 1.8562051838721366e+24\n",
      "Iteration 7: Loss = 7.913704759003266e+26\n",
      "Iteration 8: Loss = 3.373918496840408e+29\n",
      "Iteration 9: Loss = 1.4384348955496382e+32\n",
      "Iteration 10: Loss = 6.132629271207012e+34\n",
      "Iteration 11: Loss = 2.6145929748519184e+37\n",
      "Iteration 12: Loss = 1.1147111469716714e+40\n",
      "Iteration 13: Loss = 4.752492854764001e+42\n",
      "Iteration 14: Loss = 2.026196096266373e+45\n",
      "Iteration 15: Loss = 8.638579768363169e+47\n",
      "Iteration 16: Loss = 3.6830201253879802e+50\n",
      "Iteration 17: Loss = 1.5702424192993284e+53\n",
      "Iteration 18: Loss = 6.694685609341465e+55\n",
      "Iteration 19: Loss = 2.85426659590427e+58\n",
      "Iteration 20: Loss = 1.2169136102003164e+61\n",
      "Iteration 21: Loss = 5.188308909904365e+63\n",
      "Iteration 22: Loss = 2.2120389315974415e+66\n",
      "Iteration 23: Loss = 9.43106130715007e+68\n",
      "Iteration 24: Loss = 4.0209551592425484e+71\n",
      "Iteration 25: Loss = 1.7143470750637423e+74\n",
      "Iteration 26: Loss = 7.309187943347538e+76\n",
      "Iteration 27: Loss = 3.116307922590906e+79\n",
      "Iteration 28: Loss = 1.328655694864247e+82\n",
      "Iteration 29: Loss = 5.664810632160409e+84\n",
      "Iteration 30: Loss = 2.415233878817021e+87\n",
      "Iteration 31: Loss = 1.0297548318018361e+90\n",
      "Iteration 32: Loss = 4.3904530220635996e+92\n",
      "Iteration 33: Loss = 1.8719130804486628e+95\n",
      "Iteration 34: Loss = 7.981102171847313e+97\n",
      "Iteration 35: Loss = 3.4028351307546863e+100\n",
      "Iteration 36: Loss = 1.4508408978198926e+103\n",
      "Iteration 37: Loss = 6.185851844435628e+105\n",
      "Iteration 38: Loss = 2.6374247695538235e+108\n",
      "Iteration 39: Loss = 1.1245052501860361e+111\n",
      "Iteration 40: Loss = 4.794504531258725e+113\n",
      "Iteration 41: Loss = 2.044216159923888e+116\n",
      "Iteration 42: Loss = 8.715869866325342e+118\n",
      "Iteration 43: Loss = 3.7161693118750583e+121\n",
      "Iteration 44: Loss = 1.5844592770741402e+124\n",
      "Iteration 45: Loss = 6.755655628463502e+126\n",
      "Iteration 46: Loss = 2.8804129566766865e+129\n",
      "Iteration 47: Loss = 1.2281257526216047e+132\n",
      "Iteration 48: Loss = 5.236387157356789e+134\n",
      "Iteration 49: Loss = 2.2326543209333054e+137\n",
      "Iteration 50: Loss = 9.519454244012503e+139\n",
      "Iteration 51: Loss = 4.058854085965389e+142\n",
      "Iteration 52: Loss = 1.7305958190207186e+145\n",
      "Iteration 53: Loss = 7.378850021057936e+147\n",
      "Iteration 54: Loss = 3.1461725430157716e+150\n",
      "Iteration 55: Loss = 1.3414583994028145e+153\n",
      "Iteration 56: Loss = 5.719692740989275e+155\n",
      "Iteration 57: Loss = 2.4387596785256375e+158\n",
      "Iteration 58: Loss = 1.0398390614209098e+161\n",
      "Iteration 59: Loss = 4.4336771230323894e+163\n",
      "Iteration 60: Loss = 1.8904396199518967e+166\n",
      "Iteration 61: Loss = 8.060507184416535e+168\n",
      "Iteration 62: Loss = 3.4368670922666362e+171\n",
      "Iteration 63: Loss = 1.4654260736040871e+174\n",
      "Iteration 64: Loss = 6.24835790781705e+176\n",
      "Iteration 65: Loss = 2.664211431048286e+179\n",
      "Iteration 66: Loss = 1.1359841735555968e+182\n",
      "Iteration 67: Loss = 4.8436937597656046e+184\n",
      "Iteration 68: Loss = 2.0652939614894493e+187\n",
      "Iteration 69: Loss = 8.806186390286982e+189\n",
      "Iteration 70: Loss = 3.7548679670611725e+192\n",
      "Iteration 71: Loss = 1.6010403045433062e+195\n",
      "Iteration 72: Loss = 6.826697445331858e+197\n",
      "Iteration 73: Loss = 2.9108500995129632e+200\n",
      "Iteration 74: Loss = 1.2411658564711837e+203\n",
      "Iteration 75: Loss = 5.29225291645062e+205\n",
      "Iteration 76: Loss = 2.256587367398011e+208\n",
      "Iteration 77: Loss = 9.621981191168824e+210\n",
      "Iteration 78: Loss = 4.102774453424171e+213\n",
      "Iteration 79: Loss = 1.7494098534013868e+216\n",
      "Iteration 80: Loss = 7.45944093137655e+218\n",
      "Iteration 81: Loss = 3.1806931542989e+221\n",
      "Iteration 82: Loss = 1.356244692407432e+224\n",
      "Iteration 83: Loss = 5.783025539229301e+226\n",
      "Iteration 84: Loss = 2.465885737832832e+229\n",
      "Iteration 85: Loss = 1.0514571188370259e+232\n",
      "Iteration 86: Loss = 4.483435846470849e+234\n",
      "Iteration 87: Loss = 1.9117501356024893e+237\n",
      "Iteration 88: Loss = 8.151772932694657e+239\n",
      "Iteration 89: Loss = 3.475952165005368e+242\n",
      "Iteration 90: Loss = 1.4821640668004417e+245\n",
      "Iteration 91: Loss = 6.320035814942559e+247\n",
      "Iteration 92: Loss = 2.6949056665085647e+250\n",
      "Iteration 93: Loss = 1.1491278846993982e+253\n",
      "Iteration 94: Loss = 4.899975689376049e+255\n",
      "Iteration 95: Loss = 2.0893935618878765e+258\n",
      "Iteration 96: Loss = 8.90937712308887e+260\n",
      "Iteration 97: Loss = 3.799051622607186e+263\n",
      "Iteration 98: Loss = 1.619958198394258e+266\n",
      "Iteration 99: Loss = 6.90769543008023e+268\n",
      "Iteration 100: Loss = 2.9455290566393918e+271\n",
      "Final weights: [[-4.34177889e+132]\n",
      " [ 4.34266413e+132]]\n",
      "Final bias: [-7.30268248e+128]\n"
     ]
    }
   ],
   "source": [
    "init_range = 0.1\n",
    "\n",
    "weights = np.random.uniform(low=-init_range, high=init_range, size=(2, 1))\n",
    "\n",
    "bias = np.random.uniform(low=-init_range, high=init_range, size=1)\n",
    "\n",
    "learning_rate = 0.0065\n",
    "\n",
    "for i in range (100):\n",
    "    outputs = np.dot(inputs,weights) + bias\n",
    "    delta = outputs - targets\n",
    "    \n",
    "    loss = np.sum(delta ** 2)\n",
    "\n",
    "    print(f\"Iteration {i+1}: Loss = {loss}\")\n",
    "\n",
    "    delta_scaled = delta / obs\n",
    "\n",
    "    weights = weights - learning_rate * np.dot(inputs.T,delta_scaled)\n",
    "    bias = bias - learning_rate * np.sum(delta_scaled)\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3080ce-ad11-4739-b5e1-8bde1fd3ee0f",
   "metadata": {},
   "source": [
    "### A kinda-sorta(ish) example of a smoother/stabler gradient.\n",
    "This means that the gradients are neither too large (exploding) nor too small (vanishing). A stable gradient flow ensures that the model parameters are updated in a balanced manner, allowing for efficient and effective training. Here are some strategies to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c41afc-cb0b-4f01-be75-575e4da51ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
