{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7dfb16-089d-41b7-a07b-3b46d82dadb2",
   "metadata": {},
   "source": [
    "# GPT - Messing Around with Extra Neural Net Layers\n",
    "\n",
    "Goals:\n",
    "- Creating a generative pretrained tokenizer. \n",
    "- Learning the ins and outs of GPTs.\n",
    "- Adding in more layers from the base model I created earlier.\n",
    "- Experimenting with different neuron types in the layers, such as:\n",
    "    - Recurrent (long-short-term) layer, which can increase the contextual information taken up by the machine to learn and generate outputs. This helps the machine remember words, that are important to a context. \n",
    "    - Convolutional layer, which can help get more local nuances in the data. This can help detect those localised patterns in the data that aid in generating meaningful text in response to prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c8f14-9f80-4788-8039-be166a13e012",
   "metadata": {},
   "source": [
    "# Setup: Packages & Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4c1acc-12cf-419e-bb4c-d9aa13ae26c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #be smoother, use CUDA ;)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636dea6-8c28-47ae-ae9d-dc17c05aa9bd",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065210b0-c390-4a0b-8f00-cf0af5c12c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #The number of samples processed before the model is updated. Larger batch sizes can lead to faster training, but require more memory.\n",
    "block_size = 128 #The length of the sequence processed by the model. Refers to the number of tokens in each sequence.\n",
    "max_iters = 200 #The maximum number of iterations or steps the model will take during training. Each iteration updates the model's parameters based on a batch of data.\n",
    "learning_rate = 2e-5 #The step size used by the optimization algorithm to update the model's weights. A smaller learning rate can lead to more precise updates but slower convergence.\n",
    "eval_iters = 100 #The number of iterations between evaluations of the model's performance on the validation set. This helps monitor training progress and prevent overfitting.\n",
    "n_embd = 384 #The dimensionality of the embeddings used in the model. It defines the size of the vector representation for each token.\n",
    "n_head = 4 #The number of attention heads in the multi-head attention mechanism. More heads can capture different aspects of the input, but require more computational resources.\n",
    "n_layer = 4 #The number of layers (or blocks) in the model. Each layer consists of multiple sub-layers, including self-attention and feedforward layers.\n",
    "dropout = 0.2 #The dropout rate. The fraction of neurons randomly set to zero during training to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe89d5-0b84-4f5a-a826-92244a3c2b1b",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc132bb-1d94-489b-a9ef-5f913255cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_split.txt and val_split.txt have been created with a 0.8 training size.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the entire text from the file\n",
    "chars = \"\"\n",
    "with open('WizOfOz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Step 2: Define the training split size; adjust 0.8 to whatever ratio you want\n",
    "train_split = 0.8\n",
    "\n",
    "# Step 3: Calculate the split index based on the 80/20 ratio\n",
    "split_index = int(len(text) * train_split) \n",
    "\n",
    "# Step 4: Split the text into training and validation sets\n",
    "train_text = text[:split_index]\n",
    "val_text = text[split_index:]\n",
    "\n",
    "# Step 5: Write the first {train_split}% of the text to 'train_text.txt'\n",
    "train_filename = 'train_split.txt'\n",
    "with open(train_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(train_text)\n",
    "\n",
    "# Step 6: Write the remaining % of the text to 'val_text.txt'\n",
    "val_filename = 'val_split.txt'\n",
    "with open(val_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(val_text)\n",
    "\n",
    "# Confirming the files are created and written\n",
    "print(f\"{train_filename} and {val_filename} have been created with a {train_split} training size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383cb85-979b-4e30-a847-a85ea3fc6417",
   "metadata": {},
   "source": [
    "# Tokenization & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b54f7e-86a4-4f12-bc83-1a6ef2a3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) } #This line creates a dictionary called string_to_int that maps each character (ch) in the list chars to its corresponding index (i).\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) } #This line creates another dictionary called int_to_string that does the reverse of string_to_int. It maps each index (i) to its corresponding character (ch) from the list chars.\n",
    "encode = lambda s: [string_to_int[c] for c in s] #This line defines a lambda function called encode that takes a string s as input and returns a list of integers. Each character c in the string s is converted to its corresponding integer using the string_to_int dictionary.\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l]) #This line defines a lambda function called decode that takes a list of integers l as input and returns a string. Each integer i in the list l is converted to its corresponding character using the int_to_string dictionary. The characters are then joined together to form the final string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d7bc2-da3c-461b-92f7-e04a20cbcf43",
   "metadata": {},
   "source": [
    "# Memory Map  - Batching\n",
    "How the machine will use small snippets of text from a single file of any size. It's like a sampling tool.\n",
    "\n",
    "It optimises hardware use. Lets the machine open large text files without opening the entire thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f51beb6-bdbc-4d98-a6df-6d088af0ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split): #This line defines a function named get_random_chunk that takes a parameter split.\n",
    "    filename = 'train_split.txt' if split == 'train' else 'val_split.txt' #This line sets the variable filename to 'train_split.txt' if the split parameter is 'train'; otherwise, it sets it to 'val_split.txt'.\n",
    "    with open(filename, 'rb') as f: #This line opens the file specified by filename in read-binary mode ('rb') and assigns the file object to the variable f.\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm: #This line uses memory-mapped file support (mmap) to map the file into memory for read-only access. f.fileno() gets the file descriptor of the open file, and mmap.ACCESS_READ specifies read-only access.\n",
    "            file_size = len(mm) #This line gets the size of the memory-mapped file and assigns it to the variable file_size.\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size) #This line calculates a random starting position  within the file. Minus margin of the searching size\n",
    "            mm.seek(start_pos) #This line moves the file pointer to the calculated random starting position \n",
    "            block = mm.read(block_size*batch_size-1) #This line reads a block of data from the memory-mapped file starting at start_pos. The size of the block is block_size * batch_size - 1 bytes.\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '') #This line decodes the read block from bytes to a string using UTF-8 encoding, ignoring any errors during decoding. It also replaces any carriage return characters ('\\r') with an empty string.\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long) #This line encodes the decoded block to a list of integers using the encode function, then converts it to a PyTorch tensor of type long.\n",
    "        return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split) #This line calls the get_random_chunk function with the given split and assigns the returned data tensor to the variable data.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #This line generates batch_size random indices (ix) within the range from 0 to the length of data minus block_size.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #This line creates a list of slices from data of length block_size starting at each index in ix, then stacks these slices into a single tensor x.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #This line creates a list of slices from data of length block_size starting at each index in ix plus 1, then stacks these slices into a single tensor y. This represents the target data shifted by one position.\n",
    "    x, y = x.to(device), y.to(device) #This line moves the tensors x and y to the specified device (e.g., CPU or GPU).\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ada589-9413-49ff-acdf-664469538b25",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "Here are our decoders, self-attention modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e63efd62-e6ba-4b17-bbbb-03733313aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module): #A single head in a self-attention mechanism\n",
    "    \"\"\"a head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size): #This line defines the constructor (__init__) of the Head class, which initializes the object. .\n",
    "        super().__init__() #The super().__init__() call initializes the base class (nn.Module)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) #These lines define three linear layers (self.key, self.query, self.value) without biases. They transform the input embeddings (n_embd) into vectors of size head_size.\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #These layers compute the key, query, and value matrices in the self-attention mechanism.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #This line registers a buffer named tril that contains a lower triangular matrix of ones with dimensions block_size x block_size. This matrix is used to mask out future positions in the attention scores to prevent peeking into the future.\n",
    "        self.dropout = nn.Dropout(dropout) #This line initializes a dropout layer (self.dropout) with the specified dropout probability (dropout). Dropout is used to prevent overfitting by randomly setting some elements to zero during training.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape #This line unpacks the shape of the input tensor x into B (batch size), T (sequence length), and C (number of channels or embedding dimension).\n",
    "        k = self.key(x) #These lines compute the key (k) and query (q) matrices by passing the input tensor x through the respective linear layers.\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 #Computes the attention scores (wei) by performing a scaled dot-product between the query and the transpose of the key matrix. The scores are scaled by the square root of the last dimension of k to stabilize gradients.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #Masks out the upper triangular part of the attention scores (wei) to ensure that each position can only attend to previous positions and itself, preventing information leakage from future positions.\n",
    "        wei = F.softmax(wei, dim=-1) #This line applies the softmax function to the attention scores along the last dimension (dim=-1) to obtain the attention weights. The softmax function converts the scores to probabilities that sum to 1.\n",
    "        wei = self.dropout(wei) #This line applies dropout to the attention weights to prevent overfitting.\n",
    "        v = self.value(x) #This line computes the value matrix (v) by passing the input tensor x through the value linear layer.\n",
    "        out = wei @ v #This line computes the output by performing a weighted sum of the value matrix (v) using the attention weights (wei).\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c457acae-85df-49bd-8cf0-b0a7c2ea350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #This line creates a module list (self.heads) containing num_heads instances of the Head class, each with a specified head_size. Each head performs self-attention independently.\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) #This line defines a linear layer (self.proj) that projects the concatenated output of all attention heads back to the original embedding dimension (n_embd).\n",
    "        self.dropout = nn.Dropout(dropout) #This line initializes a dropout layer (self.dropout) with the specified dropout probability (dropout). Dropout is used to prevent overfitting by randomly setting some elements to zero during training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) #This line concatenates the outputs of all attention heads along the last dimension (dim=-1). Each head processes the input tensor x independently, and their outputs are combined to form a single tensor.\n",
    "        out = self.dropout(self.proj(out)) #This line applies the linear projection (self.proj) to the concatenated output, reducing it back to the original embedding dimension (n_embd). Dropout is then applied to the projected output to prevent overfitting.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddd0ef6f-0de6-4760-a507-827f72669906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a basic linear layer followed by non-linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__() #This line defines the constructor (__init__) of the FeedForward class, which initializes the object. The super().__init__() call initializes the base class (nn.Module).\n",
    "        self.net = nn.Sequential( # This line initializes a sequential container (self.net) that will hold the layers of the feedforward network. The nn.Sequential container allows for the easy stacking of layers.\n",
    "            nn.Linear(n_embd, 4 * n_embd), #This line adds a linear layer that transforms the input tensor of size n_embd to a tensor of size 4 * n_embd. This expansion is often used to increase the representational capacity of the network.\n",
    "            nn.ReLU(),                     #This line adds a ReLU (Rectified Linear Unit) activation function, which introduces non-linearity to the model. ReLU replaces all negative values in the tensor with zero.\n",
    "            nn.Linear(4 * n_embd, n_embd), #This line adds another linear layer that transforms the tensor back from size 4 * n_embd to n_embd. This ensures that the input and output dimensions of the feedforward network are the same.\n",
    "            nn.Dropout(dropout), #This line adds a dropout layer with the specified dropout probability (dropout). Dropout randomly sets a percentage of the neurons to zero during training, which helps to prevent overfitting.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61c10a2-74e6-4468-bc76-ce3a6dbd4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"transformer block: communication between nodes and computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__() #This line defines the constructor (__init__) of the Block class, which initializes the object. The super().__init__() call initializes the base class (nn.Module).\n",
    "        head_size = n_embd // n_head #This line calculates the size of each attention head (head_size) by dividing the embedding dimension (n_embd) by the number of attention heads (n_head).\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) #This line initializes a multi-head attention mechanism (self.sa) with n_head attention heads, each of size head_size.\n",
    "        self.ffwd = FeedForward(n_embd) #This line initializes a feedforward neural network (self.ffwd) with the specified embedding dimension (n_embd).\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd) #These lines initialize two layer normalization layers (self.ln1 and self.ln2), each normalizing the input to have zero mean and unit variance across the embedding dimension (n_embd).\n",
    "\n",
    "    def forward(self, x): #This line defines the forward pass method (forward) of the Block class, which takes an input tensor x.\n",
    "        y = self.sa(x) #This line passes the input tensor x through the multi-head attention mechanism (self.sa) and assigns the output to y.\n",
    "        x = self.ln1(x + y) #This line adds the output of the self-attention mechanism (y) to the original input tensor (x) and normalizes the result using the first layer normalization layer (self.ln1). This is known as the \"add and norm\" step.\n",
    "        y = self.ffwd(x) #This line passes the normalized tensor x through the feedforward neural network (self.ffwd) and assigns the output to y.\n",
    "        x = self.ln2(x + y) #This line adds the output of the feedforward network (y) to the tensor x from the previous \"add and norm\" step and normalizes the result using the second layer normalization layer (self.ln2). This is another \"add and norm\" step.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df34f07-891f-48bb-9ffb-a0762d62fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentLayer(nn.Module):\n",
    "    \"\"\"long-short-term module\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_embd, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db3f37a8-a2fe-4a0a-a0d7-41eeb7aa8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, n_embd, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv1d(n_embd, n_embd, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d5c85-3fbd-4092-9548-5f2f6fb2ac8a",
   "metadata": {},
   "source": [
    "# The Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38300e78-8655-495e-aa7a-2f5f7ff32f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefe\\AppData\\Local\\Temp\\ipykernel_22248\\2872839111.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(91, 384)\n",
       "  (position_embedding_table): Embedding(128, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (conv_layer): ConvLayer(\n",
       "    (conv): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (recurrent_layer): RecurrentLayer(\n",
       "    (lstm): LSTM(384, 384, batch_first=True)\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=91, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #This line initializes an embedding layer (self.token_embedding_table) that converts token indices into dense vectors of size n_embd. The vocabulary size is specified by vocab_size.\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #This line initializes an embedding layer (self.position_embedding_table) that converts position indices into dense vectors of size n_embd. The maximum sequence length is specified by block_size.\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) #This line initializes a sequential container (self.blocks) containing n_layer instances of the Block class. Each block performs self-attention and feedforward operations.\n",
    "        self.conv_layer = ConvLayer(n_embd)\n",
    "        self.recurrent_layer = RecurrentLayer(n_embd, hidden_size=n_embd, n_layers=1, n_head=n_head)\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #This line initializes a layer normalization layer (self.ln_f) that normalizes the final output across the embedding dimension (n_embd).\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)#This line initializes a linear layer (self.lm_head) that projects the final output back to the vocabulary size, producing logits for each token in the vocabulary.\n",
    "\n",
    "        self.apply(self._init_weights) #This line applies the _init_weights method to initialize the weights of the model.\n",
    "\n",
    "    def _init_weights(self, module): \n",
    "        if isinstance(module, nn.Linear): #This method initializes the weights of the model.\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02) #If the module is a linear layer, it initializes the weights with a normal distribution (mean 0, standard deviation 0.02) and sets the biases to zero.\n",
    "            if module.bias is not None: #If the module is an embedding layer, it also initializes the weights with a normal distribution.\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, index, targets=None): #This line defines the forward pass method (forward) of the GPTLanguageModel class, which takes input indices (index) and optional target indices (targets).\n",
    "        B, T = index.shape #This line unpacks the shape of the input tensor index into B (batch size) and T (sequence length).\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index) #This line converts the input token indices (index) into dense vectors using the token embedding table (self.token_embedding_table).\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #This line generates position indices (torch.arange(T, device=device)) and converts them into dense vectors using the position embedding table (self.position_embedding_table).\n",
    "        x = tok_emb + pos_emb #This line adds the token embeddings and positional embeddings to form the input to the neural network.\n",
    "        x = self.blocks(x) #This line passes the combined embeddings through the transformer blocks (self.blocks).\n",
    "        x = self.recurrent_layer(x)\n",
    "        x = self.ln_f(x) #This line normalizes the output of the transformer blocks using the final layer normalization layer (self.ln_f).\n",
    "        logits = self.lm_head(x) #This line projects the normalized output to the vocabulary size using the linear layer (self.lm_head), producing logits for each token.\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #If targets are provided, this block of code reshapes the logits and targets to have a shape of (B*T, C) and (B*T) respectively.\n",
    "            logits = logits.view(B*T, C) #It then computes the cross-entropy loss between the logits and targets.\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens): #This line defines the generate method, which generates new tokens. \n",
    "            index_cond = index[:, -block_size:] #It starts by extracting the last block_size tokens from the input index.\n",
    "            logits, loss = self.forward(index_cond) #This line passes the extracted tokens through the model to get the logits\n",
    "            logits = logits[:, -1, :] #This line extracts the logits corresponding to the last position.\n",
    "            probs = F.softmax(logits, dim = -1) #This line applies the softmax function to the logits to obtain probabilities for each token in the vocabulary.\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #This line samples the next token from the probability distribution using multinomial sampling.\n",
    "            index = torch.cat((index, index_next), dim=1) #This line appends the sampled token to the input index.\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size) #This line initializes an instance of the GPTLanguageModel class with the specified vocabulary size (vocab_size).\n",
    "\n",
    "# Save the model parameters\n",
    "with open('model-02.pkl', 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "\n",
    "print('model saved')\n",
    "\n",
    "# Load the model parameters\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "with open('model-02.pkl', 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f))\n",
    "print('model loaded successfully')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b984d-3dff-4892-a7f8-3f258d3b8e72",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "468bd38f-3470-4878-8255-f611bfdd07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #This decorator disables gradient calculation. It is used to reduce memory consumption and speed up computations during inference, since gradients are not needed.\n",
    "def estimate_loss():\n",
    "    out = {} #This line initializes an empty dictionary out to store the average losses for the training and validation splits.\n",
    "    model.eval() #This line sets the model to evaluation mode using model.eval(). This mode is important because it disables dropout and other training-specific behaviors.\n",
    "    for split in ['train', 'val']: #This line starts a loop that iterates over the two splits: 'train' and 'val'. This loop will estimate the loss for both the training and validation datasets.\n",
    "        losses = torch.zeros(eval_iters) #This line initializes a tensor losses with zeros, having a length equal to eval_iters. This tensor will store the loss values for each iteration.\n",
    "        for k in range(eval_iters): #This nested loop runs for eval_iters iterations. In each iteration:\n",
    "            X, Y = get_batch(split) #Retrieves a batch of input data (X) and target data (Y) for the given split.\n",
    "            logits, loss = model(X, Y) #Computes the logits and loss for the batch using the model.\n",
    "            losses[k] = loss.item() #Stores the computed loss in the losses tensor at index k.\n",
    "        out[split] = losses.mean() #This line calculates the mean of the losses tensor for the current split and stores it in the out dictionary with the split name as the key.\n",
    "    model.train() #This line sets the model back to training mode using model.train(). This mode re-enables dropout and other training-specific behaviors.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4df81f9-742d-4eb9-aa1e-5d9573dfec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, train loss:4.580, val loss:4.570\n",
      "step:100, train loss:3.003, val loss:3.114\n",
      "2.608344078063965\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #This line initializes an AdamW optimizer with the parameters of the model (model.parameters()) and a specified learning rate (learning_rate). AdamW is a variant of the Adam optimizer with weight decay regularization.\n",
    "\n",
    "for iter in range(max_iters): #This line starts a loop that iterates from 0 to max_iters-1, where max_iters is the total number of iterations for training.\n",
    "    if iter % eval_iters == 0: #Inside the loop, this block of code checks if the current iteration (iter) is a multiple of eval_iters. \n",
    "        losses = estimate_loss() #If true, it estimates the training and validation losses using the estimate_loss function and prints the current iteration step along with the losses.\n",
    "        print(f\"step:{iter}, train loss:{losses['train']:.3f}, val loss:{losses['val']:.3f}\")\n",
    "\n",
    "    xb, yb = get_batch('train') #This line retrieves a batch of input data (xb) and target data (yb) for training using the get_batch function.\n",
    "\n",
    "    logits, loss = model.forward(xb, yb) #This line performs a forward pass through the model with the training batch (xb, yb) and computes the logits and loss.\n",
    "    optimizer.zero_grad(set_to_none=True) #This line clears the gradients of all optimized parameters by setting them to None. This is a more memory-efficient way to zero the gradients compared to setting them to zero.\n",
    "    loss.backward() #This line computes the gradient of the loss with respect to the model parameters using backpropagation.\n",
    "    optimizer.step() #This line updates the model parameters based on the computed gradients using the optimizer.\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-02.pkl', 'wb') as f: #This block of code opens a file named 'model-02.pkl' in write-binary mode ('wb') and saves the model parameters to the file using pickle.dump.\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c97bfb9-4822-4e45-9dba-d8e796b154e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mo' ok traolyOkkjrochar5eJ, .\n",
      "\n",
      "\n",
      "”rera doda t hosWuprhenPtuanb.\n",
      "lmrei”t, fer a,tepu'\n",
      "\n",
      "uq: ol\n",
      "_,x s•ing tenfnait “b1ingned ZrBou aEanenePcese rasoum’lfe tapm tRhe _idusu utpmd t\" whee ga$ ithinal- t Rin \n",
      "hidar’it\n",
      "scchiYe. unore n﻿oved the we$3er_Xed,  tho anpdhurshedrGe bFta pis toneJ eXrwree maro.yUc ro fusotat !aly itged Ict o—'ll“ly sano tres st 7o uren het ce,r wome souvg,id\n",
      "\n",
      "We th Sotche we fot95  atheed thro\n",
      " wewe\":\n",
      "*auno?Ber o tu4) caLcy pek yy tis‘7 _a, pthernre4.e her ged,\n",
      "nDe f?o wroanMo\n"
     ]
    }
   ],
   "source": [
    "prompt = \" \"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e19482-9a11-4dd4-9216-05092a8544de",
   "metadata": {},
   "source": [
    "# A Chat Bot Loop\n",
    "So I've trained a (rather underwhelming!!) chatbot. \n",
    "\n",
    "This is example code of how to interact with it. \n",
    "\n",
    "I'm not going to make a .py file for it to execute because it isn't worth it, but to show how to integrate. \n",
    "\n",
    "Enjoy this capability in Jupyter notebook for now. Otherwise, visit: <<https://chatgpt.com/>> for a more robust experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dd9ae-3809-498a-a3b4-79da87179eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " lo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete:\n",
      "lood&yprUy fomeh \"er2nsp n-osragahe Woutilt indre ud)eangd 5le\n",
      "\n",
      "\" bevethe Uaut weabr\"\n",
      "4e\"\n",
      "ql Re  Giniw Van tBet ouur theut s sadtn!he t7e the1 tOid, ;soarlkeuaI ,Joy heq nn t Nm.\n",
      "d\n",
      "Tl-hothal eareh, anbe\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete:\n",
      "hiyburlac\n",
      "eao theind u‘tremHof ang  foorShaogW arock the2and\n",
      "\"\n",
      "f'ret6fj &or ad *moa. \"me (l\n",
      "yef a.™dQrJsble scqe‘ awr‘e ﻿rh\n",
      "e to nnd the ty’ faOd™ey szalad waU re2sathe wsute horgA the;erxerundQnt Jte t\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " low\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete:\n",
      "lowhepd i.0toub tithleH 1of\"reyd the the ?ne, akt hhu anayO erd\n",
      "o\n",
      "t \"omu thes soLhuisdi t ot]\"e fre gal tPhex ravy“ w7asi 2ed™ma toD﻿”k39y nleole sewrewely4 Re ne. fu tFan1ov atlerl%u 0ted ge\"(in9 do siL\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = input(\"prompt (type 'exit' to quit):\\n\")\n",
    "    if prompt.lower() == 'exit':\n",
    "        print(\"Shutting down...\")\n",
    "        break\n",
    "\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    generated_tokens = model.generate(context, max_new_tokens=200)[0].tolist()\n",
    "    generated_chars = decode(generated_tokens)\n",
    "    print(f'complete:\\n{generated_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eaf0a-9fa2-4024-b9dd-e0ead8101ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70307fd-a3ae-4ee9-bcff-b3ac6bfbe093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21118563-e2ae-4a65-8a00-e024680e2cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa8b31-7d14-4c93-bdfa-8daad686c644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684830e3-a980-4007-8f6d-0c8bf2e178c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf05f6-50e0-4863-b13e-4035b15e0ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78b42c-8e50-46b5-af3f-6ab92b889011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf5c33-48dd-4ece-87b9-bcea14d671e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
