{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61257244",
   "metadata": {},
   "source": [
    "Context: using Wizard of Oz (for now) to train an LLM. \n",
    "\n",
    "Why am I doing this? To better understand how LLMs work, understand the math involved and the programming required. This will help me to understand LLM use cases, their limitations, and hopefully, their commercial application moving forward. \n",
    "\n",
    "I'm highly interested in NLP, AI, etc., so this project will serve as a foundational base to demonstrate some skills I'm acquring and looking to deploy in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee405dae",
   "metadata": {},
   "source": [
    "# Setup and Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d4a096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#these are important hyperparamters for training and learning patterns of language\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efad69a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250472\n"
     ]
    }
   ],
   "source": [
    "with open('WizOfOz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(len(text)) #print length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb51db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(chars) #show all character parts\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0a484",
   "metadata": {},
   "source": [
    "# Character-level Tokenisation\n",
    "Taking each character and turning it into an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad1b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "data = torch.tensor(encode(text), dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89231701",
   "metadata": {},
   "source": [
    "# Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e3b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 3,  1, 76, 64, 61,  1, 63, 65],\n",
      "        [47, 64, 61,  1, 76, 74, 61, 61],\n",
      "        [69, 61, 75,  1, 58, 61, 62, 71],\n",
      "        [78, 61,  1, 78, 61, 74, 81,  1]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[ 1, 76, 64, 61,  1, 63, 65, 74],\n",
      "        [64, 61,  1, 76, 74, 61, 61, 12],\n",
      "        [61, 75,  1, 58, 61, 62, 71, 74],\n",
      "        [61,  1, 78, 61, 74, 81,  1, 75]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data)) #80% train split\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#batching \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f3483",
   "metadata": {},
   "source": [
    "# Tensor Batching for Predictions\n",
    "\n",
    "Part of generating novel text is allowing the computer to make predictions on context. Making a batch of tokens as a basis of prediction, and then being able to look before/after that batch allows it to start building a set of predictions. The idea is not for the code to simply regurgitate the Wizard of Oz, but to actually make something novel. This requires understanding the context of the tokens. And that requires understanding the tokens in meaningful batches, forward and backward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8083994",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #makes sure that gradients aren't used - improving performance in training \n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() #puts the machine into training model, making it less likely to overfit (dropping neurons, weights, etc.) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "070443ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B\n",
      ",;s![33BOH$;gix)sU$Nmf/yf[wh2:'El)LUZX:’]nDFNxa‘W'rK*;T/?j8C_drMa-fbmj/b/g-QI%F”J”’uryI_M•b57'Xp*UD6k6mcn.‘”.c_JGek —3nZXl—’7“EoT\n",
      "z8Xq﻿A3i)llXhQ]e-h7PAT;SvY50cZ!—S“v’v\"(’mqy5$\"cI_j/4I p﻿rb[TB&M'E1z%IwN1Igv!g\n",
      "IB[f?KFMBPygB&eV'smU9“A“\n",
      "]_mhPb/fOYBipVOfgVr\n",
      "]nP6”Y$Ah[7*y;TTg4X8o•4gFa-c\"LOHEN5“vpL&.Fvyh’PkxaWE—F﻿Yb™*T?]KyHfn*u?$w2[m_4)2bPB7“qLWdpB;jo:/ mi%)Y3t&yY1Lcq*4q)s.A?Gs\n",
      "•7uix/’I 2T?;F67Vntl(z%,/Yzo“D“R™hVU™ZrMRV-lBIkrMS]9•''MDA$,wekZ-yvgcIt™i991bYkJ;I[”_cv\"]eR_cIfq‘N5“vdmW?4’\"VsJ,?‘9/3tX5™HKV\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens): #index is (B, T) array of indices of the current context\n",
    "        for _ in range(max_new_tokens): #gets the predictions\n",
    "            logits, loss = self.forward(index) #focus only on the last time steps\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) #apply softmax to normalise these batches into probabilities \n",
    "            index_next = torch.multinomial(probs, num_samples=1) # sample from the distribution\n",
    "            index = torch.cat((index, index_next), dim=1) #append the sampled index to the running sequence being generated \n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "\n",
    "# this creates a giant grid, and will search from start to finish, making predictions for the next one after\n",
    "#logits are distributions of probabilities. A follows B is x, A follows C is x, etc... all based on probabilities of their occurrence, stored in vector/matrix form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47af76-b9ac-4ee7-ac8f-6a1db662c592",
   "metadata": {},
   "source": [
    "#### What is this?\n",
    "The first run of the Neural Net trying to make sense of characters. We could call this a default state. Based on untrained data, these are the kinds of predictions it makes. They are bad. But it's a useful reference point to note of what a bigram model does under the hood with no training or validation. It's not being told to improve, just use unoptimised weights to predict. It'd be like asking someone who can't read English to make sense of a text. They'd just see gibberish and maybe associate some combination of letters/words with a language they already know (let's say Hungarian) and be like, \"hey that's meaningful there because I know it based on my understanding of a language\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed46a5-270d-4313-8084-0cff032fd0d9",
   "metadata": {},
   "source": [
    "# Training, Optimising, Validation\n",
    "\n",
    "As opposed to the output above, which is the \"default state\", below is code making the neural net learn the bigram associations between words/characters in the text being parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad5479a-d2b4-4235-8403-74f9d88a1d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, train loss: 2.674, val loss: 2.840\n",
      "step:250, train loss: 2.660, val loss: 2.790\n",
      "step:500, train loss: 2.653, val loss: 2.819\n",
      "step:750, train loss: 2.674, val loss: 2.815\n",
      "step:1000, train loss: 2.669, val loss: 2.846\n",
      "step:1250, train loss: 2.627, val loss: 2.831\n",
      "step:1500, train loss: 2.650, val loss: 2.807\n",
      "step:1750, train loss: 2.642, val loss: 2.824\n",
      "step:2000, train loss: 2.640, val loss: 2.802\n",
      "step:2250, train loss: 2.623, val loss: 2.772\n",
      "step:2500, train loss: 2.628, val loss: 2.781\n",
      "step:2750, train loss: 2.646, val loss: 2.800\n",
      "step:3000, train loss: 2.628, val loss: 2.765\n",
      "step:3250, train loss: 2.605, val loss: 2.793\n",
      "step:3500, train loss: 2.610, val loss: 2.765\n",
      "step:3750, train loss: 2.590, val loss: 2.755\n",
      "step:4000, train loss: 2.589, val loss: 2.786\n",
      "step:4250, train loss: 2.588, val loss: 2.768\n",
      "step:4500, train loss: 2.579, val loss: 2.766\n",
      "step:4750, train loss: 2.579, val loss: 2.733\n",
      "step:5000, train loss: 2.579, val loss: 2.766\n",
      "step:5250, train loss: 2.578, val loss: 2.772\n",
      "step:5500, train loss: 2.575, val loss: 2.734\n",
      "step:5750, train loss: 2.570, val loss: 2.767\n",
      "step:6000, train loss: 2.560, val loss: 2.730\n",
      "step:6250, train loss: 2.581, val loss: 2.741\n",
      "step:6500, train loss: 2.562, val loss: 2.759\n",
      "step:6750, train loss: 2.548, val loss: 2.749\n",
      "step:7000, train loss: 2.522, val loss: 2.764\n",
      "step:7250, train loss: 2.541, val loss: 2.758\n",
      "step:7500, train loss: 2.560, val loss: 2.741\n",
      "step:7750, train loss: 2.544, val loss: 2.716\n",
      "step:8000, train loss: 2.514, val loss: 2.693\n",
      "step:8250, train loss: 2.507, val loss: 2.748\n",
      "step:8500, train loss: 2.541, val loss: 2.768\n",
      "step:8750, train loss: 2.539, val loss: 2.736\n",
      "step:9000, train loss: 2.543, val loss: 2.719\n",
      "step:9250, train loss: 2.553, val loss: 2.740\n",
      "step:9500, train loss: 2.511, val loss: 2.758\n",
      "step:9750, train loss: 2.544, val loss: 2.677\n",
      "2.5610454082489014\n"
     ]
    }
   ],
   "source": [
    "# training loop with optimizer \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step:{iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "        \n",
    "    xb, yb = get_batch('train') #sampling a batch of data \n",
    "\n",
    "    logits, loss = model.forward(xb, yb) #loss evaluation\n",
    "    optimizer.zero_grad(set_to_none=True) #making sure the gradient descent function isn't using previous gradients, only the current batch\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8cbf2a-5516-431c-bdb4-0d89baeb7a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Macouthed re cu\n",
      "on\n",
      "scat berofthed pue:jurerexokDplus arotoue bes St hetashenb, w e t?)!\"G2Alere l\n",
      "WTofe, V(-7SFIte ed usmemankincou_$remedon therayo at f\n",
      "nured\n",
      "anon I wno hen foypxnthia ang Nthy!Glk,\"\n",
      "TofOBilize to h thedr bount  thex1T' at ber\n",
      "OC™qmple C0Qran he Shor\n",
      "cldles;*B-'d ubP4K1m, a coreren'sis?athes y,Whe;je, t\n",
      "\"N(otirthen w. aus ithe\n",
      "Hb;ry f﻿, d rebl, bl JRAGlly OP(Ift ang ckirin ty\n",
      "mPNe h piberedssthitthe.\n",
      "\n",
      "TTRALKisarar ss is W5, t swana,\"VOF]’tte at a\n",
      "isthane s  me, at msimeig t?“I\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8bbb2-914f-4796-ad71-3eca77e440af",
   "metadata": {},
   "source": [
    "### What is this?\n",
    "This is post-training and validation. \n",
    "Some things comparing to the first output: \n",
    "- There are more line breaks. It is predicting paragraph size. This is important. \n",
    "- Next, there are some emergent basic word-like combinations coming. There are more vowel-consonant combinations.\n",
    "- We could re-run this chunk of text repeatedly to reduce loss and output some more \"word-ish\" outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a536bb-840a-4a5e-acee-cab105f9e9da",
   "metadata": {},
   "source": [
    "### Optimizers \n",
    "Optimizers help a neural net know if it is going in the right direction. They're all based on linear algebra and statistical learning principles.\n",
    "\n",
    "### Common optimizers:\n",
    "1. **Mean square error (MSE).** Common loss function. Used to minimise prediction errors. The goal is to predict a continuous output (e.g., prices) it measures the averaged squared difference between the predicted and actual values. Common for regression neural network tasks. It penalises larger errors more than smaller ones due to the squaring. \n",
    "2. **Gradient Descent (GD).** Used to minimise the loss function in a neural network. It measures how well the model is able to predict the target based on input features. It finds the minimum of the loss function by updating the model's parameters. It is an iterative process, with the machine learning which direction to take a step in in order to reduce the loss, in the steepest increments possible. There are variants like Batch GD (uses the whole dataset), Stochastic GD (uses one sample at a time), and Mini-batch GD (uses a subset of the data)\n",
    "3. **Momentum.** Extension of gradient descent. It adds a momentum parameter which helps smooth out the updates to the loss function optimisation route the machine is taking. It helps the optimizer continue moving in the same direction, even if the gradient changes. It helps the machine by also optimising for multidirectional gradient descent or different magnitudes. Momentum can help avoid local minima and accelerate convergence making it useful for deep learning.\n",
    "4. **RMSProp.** An optimisation algorithm based on moving averages of the squared gradient it adapts the learning rate for each parameter individually. This helps stabilise the machine by avoiding big swings or jumps in the gradient descent. It can help improve convergence. RMSProp is particularly useful in recurrent neural networks and for training on mini-batches.\n",
    "5. **Adam.** Popular that combines momentum and RMSProp. It uses the moving average of both gradient and its squared value to adapt learning rates for each parameter. It is usually the default optimiser for deep learning models.\n",
    "6. **AdamW.** Modification of the Adam optimiser. It adds weight decay to paramters. This helps make the model smoother and adapt to more generalised use cases, and some weights may be overfitted. Weight decay helps prevent overfitting by discouraging overly complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea9ecb-2895-424b-8f9d-bae559f02919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
