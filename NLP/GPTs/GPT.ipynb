{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7dfb16-089d-41b7-a07b-3b46d82dadb2",
   "metadata": {},
   "source": [
    "# GPT - from Scratch\n",
    "\n",
    "Goals:\n",
    "- Creating a generative pretrained tokenizer. \n",
    "- Learning the ins and outs of GPTs.\n",
    "\n",
    "Notes:\n",
    "- Trained on the Wizard of Oz, downloaded here: <<https://www.gutenberg.org/ebooks/55>> download in UTF-8 format\n",
    "- Do not expecting any spectacular results. This \n",
    "- I'm doing this to understand Neural Nets, transformers, math, etc., so taking a practical deep dive into what it takes.\n",
    "\n",
    "General architecture:\n",
    "1. **Tokenized inputs:** chunking the input text data into parts\n",
    "2. **Embeddings and positional encoding:** convert tokens into embeddings (vectors/tensors), and account for their relative locations\n",
    "3. **Decoder x 4, comprised of:**\n",
    "   - Multihead attention. made up of:\n",
    "       - Key x Querey x Value (from emebddings)\n",
    "       - scaled dot product of attention, which is:\n",
    "         - dot product of K Q V, scale by 1/sqrt(len(row of keys or queries or matrix)) - prevent vanishing gradient problem\n",
    "         - torch.tril for masking to prevent the model looking forward, instead basing its predictions on the past (to prevent overfitting)\n",
    "         - softmax transform\n",
    "         - matrix multiply\n",
    "       - concat results of the attention heads\n",
    "       - linear transform to combine the attention ouputs\n",
    "   - Add a residual connection from the multihead attention to its output\n",
    "   - Normalise the result\n",
    "   - Feedforward\n",
    "     - Apply a linear transform\n",
    "     - Use a non-linear activation function, in this case ReLU\n",
    "     - Apply another linear transform\n",
    "   - Residual connection, then normalise\n",
    "5. **Output layer**:\n",
    "    - Applying a final linear transformer\n",
    "    - Softmax transform to attain probabilistic samplings: this samples the next token based on probabilities \n",
    "9. **Comparison to targets (backpropagation)**: computing the loss by comparing predictions to targets, and updating model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53849abe-4542-4a2a-bb14-d21821251054",
   "metadata": {},
   "source": [
    "Chat-GPT4 has approximately 1.76tn parameters. Source: <<https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/>>\n",
    "Parameters are calculated by multiplying and adding the various layers of neurons found in the neural net powering the model. \n",
    "\n",
    "Here's the rough guide on finding the parameter size of this model:\n",
    "- Embedding layers:\n",
    "    - self.token_embedding_table: This layer has vocab_size * n_embd parameters.\n",
    "    - self.position_embedding_table: This layer has block_size * n_embd parameters.\n",
    "- Transformer layer:\n",
    "    - Multi-attention layer:\n",
    "        - Total for all heads: num_heads * (3 * n_embd * head_size) + n_embd * n_embd.\n",
    "    - Feedforward layer:\n",
    "        - n_embd * 4 * n_embd + 4 * n_embd * n_embd.\n",
    "- Normalization layer\n",
    "    - Has 2 * n_embd parameters\n",
    "- Output layer\n",
    "    - sThis layer has n_embd * vocab_size parameters.\n",
    " \n",
    "Note:\n",
    "- vocab_size = 100\n",
    "- n_embd = 384\n",
    "- n_head = 4\n",
    "- n_layer = 4\n",
    "- block_size = 128\n",
    "\n",
    "Meaning the model has 7209984 parameters. Calculation done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30b6e2a-5eb7-4d50-a41d-6823cccef524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7209984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given values\n",
    "vsize = 100  # Vocabulary size\n",
    "n_embd = 384  # Dimensionality of the embeddings\n",
    "n_head = 4  # Number of attention heads\n",
    "n_layer = 4  # Number of layers\n",
    "bsize = 128  # Sequence length\n",
    "\n",
    "# Calculating the number of parameters for each part\n",
    "token_embedding_params = vsize * n_embd\n",
    "position_embedding_params = bsize * n_embd\n",
    "\n",
    "# Multi-Head Attention parameters per block\n",
    "attention_params_per_block = 3 * (n_embd * n_embd) + (n_embd * n_embd)\n",
    "\n",
    "# Feed-Forward Layer parameters per block\n",
    "feedforward_params_per_block = n_embd * (4 * n_embd) + (4 * n_embd) * n_embd\n",
    "\n",
    "# Layer Normalization parameters per block\n",
    "layer_norm_params_per_block = 2 * n_embd\n",
    "\n",
    "# Total parameters per Transformer block\n",
    "total_params_per_block = (\n",
    "    attention_params_per_block + \n",
    "    feedforward_params_per_block + \n",
    "    2 * layer_norm_params_per_block\n",
    ")\n",
    "\n",
    "# Total parameters for all Transformer blocks\n",
    "total_params_all_blocks = n_layer * total_params_per_block\n",
    "\n",
    "# Output Linear Layer parameters\n",
    "output_linear_params = n_embd * vsize\n",
    "\n",
    "# Total number of parameters\n",
    "total_params = (\n",
    "    token_embedding_params +\n",
    "    position_embedding_params +\n",
    "    total_params_all_blocks +\n",
    "    output_linear_params\n",
    ")\n",
    "\n",
    "total_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0a91-514e-4ff4-a71f-d3766c3c94db",
   "metadata": {},
   "source": [
    "Here is a visual of the hierarchy of operations going on in this code:\n",
    "\n",
    "    **Libraries and Device Setup**\n",
    "     ├── Import libraries\n",
    "     └── Set device\n",
    "          └── 'cuda' if available, otherwise 'cpu'\n",
    "    \n",
    "    **Hyperparameters**\n",
    "     ├── Define batch_size, block_size, etc.\n",
    "    \n",
    "    **Data Preparation**\n",
    "     ├── Read text file\n",
    "          └── Read entire text from the file\n",
    "     ├── Split text into train and val sets\n",
    "          ├── Calculate split ratio\n",
    "          ├── Split text based on the split index\n",
    "          └── Write split text to separate files\n",
    "     └── Encode and decode functions\n",
    "          ├── Encode: Convert characters to integers\n",
    "          └── Decode: Convert integers to characters\n",
    "    \n",
    "    **Batch Preparation**\n",
    "     ├── Extract random chunks of text\n",
    "          ├── Define function to get random chunk from train/val file\n",
    "          └── Use memory-mapped file support for efficient reading\n",
    "     └── Create batches of input and target data\n",
    "          ├── Generate random indices for batch\n",
    "          ├── Create input tensor `x`\n",
    "          └── Create target tensor `y`\n",
    "    \n",
    "    **Model Definition**\n",
    "     ├── Head\n",
    "          └── Single head in a self-attention mechanism\n",
    "     ├── MultiHeadAttention\n",
    "          └── Multiple heads of self-attention in parallel\n",
    "     ├── FeedForward\n",
    "          └── Basic linear layer followed by non-linear layer\n",
    "     ├── Block\n",
    "          └── Combines multi-head attention and feedforward with normalization\n",
    "     └── GPTLanguageModel\n",
    "          ├── Embedding layers for tokens and positions\n",
    "          ├── Sequential container for transformer blocks\n",
    "          ├── Layer normalization\n",
    "          └── Linear projection\n",
    "    \n",
    "    **Model Initialization and Training**\n",
    "     ├── Load model parameters\n",
    "          └── Load pre-trained model parameters from file\n",
    "     ├── Estimate loss\n",
    "          ├── Define function to estimate loss on train and val sets\n",
    "          └── Compute loss and store in dictionary\n",
    "     ├── Setup optimizer\n",
    "          └── Initialize AdamW optimizer\n",
    "     └── Training loop\n",
    "          ├── Iteratively train model\n",
    "          ├── Compute loss and update model parameters\n",
    "          └── Print training and validation loss\n",
    "    \n",
    "    **Text Generation**\n",
    "     ├── Generate method\n",
    "          ├── Extract last block_size tokens\n",
    "          ├── Pass through model to get logits\n",
    "          ├── Apply softmax to get probabilities\n",
    "          ├── Sample next token\n",
    "          └── Append sampled token to input\n",
    "     └── Interactive prompt\n",
    "          ├── Allow interactive text generation with user input\n",
    "          └── Continue generating tokens based on user prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c8f14-9f80-4788-8039-be166a13e012",
   "metadata": {},
   "source": [
    "# Setup: Packages & Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4c1acc-12cf-419e-bb4c-d9aa13ae26c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #be smoother, use CUDA ;)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636dea6-8c28-47ae-ae9d-dc17c05aa9bd",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "065210b0-c390-4a0b-8f00-cf0af5c12c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #The number of samples processed before the model is updated. Larger batch sizes can lead to faster training, but require more memory.\n",
    "block_size = 128 #The length of the sequence processed by the model. Refers to the number of tokens in each sequence.\n",
    "max_iters = 200 #The maximum number of iterations or steps the model will take during training. Each iteration updates the model's parameters based on a batch of data.\n",
    "learning_rate = 2e-5 #The step size used by the optimization algorithm to update the model's weights. A smaller learning rate can lead to more precise updates but slower convergence.\n",
    "eval_iters = 100 #The number of iterations between evaluations of the model's performance on the validation set. This helps monitor training progress and prevent overfitting.\n",
    "n_embd = 384 #The dimensionality of the embeddings used in the model. It defines the size of the vector representation for each token.\n",
    "n_head = 4 #The number of attention heads in the multi-head attention mechanism. More heads can capture different aspects of the input, but require more computational resources.\n",
    "n_layer = 4 #The number of layers (or blocks) in the model. Each layer consists of multiple sub-layers, including self-attention and feedforward layers.\n",
    "dropout = 0.2 #The dropout rate. The fraction of neurons randomly set to zero during training to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe89d5-0b84-4f5a-a826-92244a3c2b1b",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc132bb-1d94-489b-a9ef-5f913255cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_split.txt and val_split.txt have been created with a 0.8 training size.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the entire text from the file\n",
    "chars = \"\"\n",
    "with open('WizOfOz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Step 2: Define the training split size; adjust 0.8 to whatever ratio you want\n",
    "train_split = 0.8\n",
    "\n",
    "# Step 3: Calculate the split index based on the ratio given above\n",
    "split_index = int(len(text) * train_split) \n",
    "\n",
    "# Step 4: Split the text into training and validation sets\n",
    "train_text = text[:split_index]\n",
    "val_text = text[split_index:]\n",
    "\n",
    "# Step 5: Write the train split % of the text to 'train_text.txt'\n",
    "train_filename = 'train_split.txt'\n",
    "with open(train_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(train_text)\n",
    "\n",
    "# Step 6: Write the remaining % of the text to 'val_text.txt'\n",
    "val_filename = 'val_split.txt'\n",
    "with open(val_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(val_text)\n",
    "\n",
    "# Confirming the files are created and written\n",
    "print(f\"{train_filename} and {val_filename} have been created with a {train_split} training size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383cb85-979b-4e30-a847-a85ea3fc6417",
   "metadata": {},
   "source": [
    "# Tokenization & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b54f7e-86a4-4f12-bc83-1a6ef2a3bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(chars) } #This line creates a dictionary called string_to_int that maps each character (ch) in the list chars to its corresponding index (i).\n",
    "int_to_string = { i:ch for i, ch in enumerate(chars) } #This line creates another dictionary called int_to_string that does the reverse of string_to_int. It maps each index (i) to its corresponding character (ch) from the list chars.\n",
    "encode = lambda s: [string_to_int[c] for c in s] #This line defines a lambda function called encode that takes a string s as input and returns a list of integers. Each character c in the string s is converted to its corresponding integer using the string_to_int dictionary.\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l]) #This line defines a lambda function called decode that takes a list of integers l as input and returns a string. Each integer i in the list l is converted to its corresponding character using the int_to_string dictionary. The characters are then joined together to form the final string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d7bc2-da3c-461b-92f7-e04a20cbcf43",
   "metadata": {},
   "source": [
    "# Memory Map  - Batching\n",
    "How the machine will use small snippets of text from a single file of any size. It's like a sampling tool.\n",
    "\n",
    "It optimises hardware use. Lets the machine open large text files without opening the entire thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f51beb6-bdbc-4d98-a6df-6d088af0ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(split): #This line defines a function named get_random_chunk that takes a parameter split.\n",
    "    filename = 'train_split.txt' if split == 'train' else 'val_split.txt' #This line sets the variable filename to 'train_split.txt' if the split parameter is 'train'; otherwise, it sets it to 'val_split.txt'.\n",
    "    with open(filename, 'rb') as f: #This line opens the file specified by filename in read-binary mode ('rb') and assigns the file object to the variable f.\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm: #This line uses memory-mapped file support (mmap) to map the file into memory for read-only access. f.fileno() gets the file descriptor of the open file, and mmap.ACCESS_READ specifies read-only access.\n",
    "            file_size = len(mm) #This line gets the size of the memory-mapped file and assigns it to the variable file_size.\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size) #This line calculates a random starting position  within the file. Minus margin of the searching size\n",
    "            mm.seek(start_pos) #This line moves the file pointer to the calculated random starting position \n",
    "            block = mm.read(block_size*batch_size-1) #This line reads a block of data from the memory-mapped file starting at start_pos. The size of the block is block_size * batch_size - 1 bytes.\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '') #This line decodes the read block from bytes to a string using UTF-8 encoding, ignoring any errors during decoding. It also replaces any carriage return characters ('\\r') with an empty string.\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long) #This line encodes the decoded block to a list of integers using the encode function, then converts it to a PyTorch tensor of type long.\n",
    "        return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split) #This line calls the get_random_chunk function with the given split and assigns the returned data tensor to the variable data.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #This line generates batch_size random indices (ix) within the range from 0 to the length of data minus block_size.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #This line creates a list of slices from data of length block_size starting at each index in ix, then stacks these slices into a single tensor x.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #This line creates a list of slices from data of length block_size starting at each index in ix plus 1, then stacks these slices into a single tensor y. This represents the target data shifted by one position.\n",
    "    x, y = x.to(device), y.to(device) #This line moves the tensors x and y to the specified device (e.g., CPU or GPU).\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ada589-9413-49ff-acdf-664469538b25",
   "metadata": {},
   "source": [
    "# Neural Nets\n",
    "Here are our decoders, self-attention modules, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63efd62-e6ba-4b17-bbbb-03733313aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module): #A single head in a self-attention mechanism\n",
    "    \"\"\"a head of self-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, head_size): #This line defines the constructor (__init__) of the Head class, which initializes the object. .\n",
    "        super().__init__() #The super().__init__() call initializes the base class (nn.Module)\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False) #These lines define three linear layers (self.key, self.query, self.value) without biases. They transform the input embeddings (n_embd) into vectors of size head_size.\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False) #These layers compute the key, query, and value matrices in the self-attention mechanism.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #This line registers a buffer named tril that contains a lower triangular matrix of ones with dimensions block_size x block_size. This matrix is used to mask out future positions in the attention scores to prevent peeking into the future.\n",
    "        self.dropout = nn.Dropout(dropout) #This line initializes a dropout layer (self.dropout) with the specified dropout probability (dropout). Dropout is used to prevent overfitting by randomly setting some elements to zero during training.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape #This line unpacks the shape of the input tensor x into B (batch size), T (sequence length), and C (number of channels or embedding dimension).\n",
    "        k = self.key(x) #These lines compute the key (k) and query (q) matrices by passing the input tensor x through the respective linear layers.\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 #Computes the attention scores (wei) by performing a scaled dot-product between the query and the transpose of the key matrix. The scores are scaled by the square root of the last dimension of k to stabilize gradients.\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) #Masks out the upper triangular part of the attention scores (wei) to ensure that each position can only attend to previous positions and itself, preventing information leakage from future positions.\n",
    "        wei = F.softmax(wei, dim=-1) #This line applies the softmax function to the attention scores along the last dimension (dim=-1) to obtain the attention weights. The softmax function converts the scores to probabilities that sum to 1.\n",
    "        wei = self.dropout(wei) #This line applies dropout to the attention weights to prevent overfitting.\n",
    "        v = self.value(x) #This line computes the value matrix (v) by passing the input tensor x through the value linear layer.\n",
    "        out = wei @ v #This line computes the output by performing a weighted sum of the value matrix (v) using the attention weights (wei).\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c457acae-85df-49bd-8cf0-b0a7c2ea350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #This line creates a module list (self.heads) containing num_heads instances of the Head class, each with a specified head_size. Each head performs self-attention independently.\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) #This line defines a linear layer (self.proj) that projects the concatenated output of all attention heads back to the original embedding dimension (n_embd).\n",
    "        self.dropout = nn.Dropout(dropout) #This line initializes a dropout layer (self.dropout) with the specified dropout probability (dropout). Dropout is used to prevent overfitting by randomly setting some elements to zero during training.\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) #This line concatenates the outputs of all attention heads along the last dimension (dim=-1). Each head processes the input tensor x independently, and their outputs are combined to form a single tensor.\n",
    "        out = self.dropout(self.proj(out)) #This line applies the linear projection (self.proj) to the concatenated output, reducing it back to the original embedding dimension (n_embd). Dropout is then applied to the projected output to prevent overfitting.\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddd0ef6f-0de6-4760-a507-827f72669906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a basic linear layer followed by non-linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__() #This line defines the constructor (__init__) of the FeedForward class, which initializes the object. The super().__init__() call initializes the base class (nn.Module).\n",
    "        self.net = nn.Sequential( # This line initializes a sequential container (self.net) that will hold the layers of the feedforward network. The nn.Sequential container allows for the easy stacking of layers.\n",
    "            nn.Linear(n_embd, 4 * n_embd), #This line adds a linear layer that transforms the input tensor of size n_embd to a tensor of size 4 * n_embd. This expansion is often used to increase the representational capacity of the network.\n",
    "            nn.ReLU(),                     #This line adds a ReLU (Rectified Linear Unit) activation function, which introduces non-linearity to the model. ReLU replaces all negative values in the tensor with zero.\n",
    "            nn.Linear(4 * n_embd, n_embd), #This line adds another linear layer that transforms the tensor back from size 4 * n_embd to n_embd. This ensures that the input and output dimensions of the feedforward network are the same.\n",
    "            nn.Dropout(dropout), #This line adds a dropout layer with the specified dropout probability (dropout). Dropout randomly sets a percentage of the neurons to zero during training, which helps to prevent overfitting.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61c10a2-74e6-4468-bc76-ce3a6dbd4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"transformer block: communication between nodes and computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__() #This line defines the constructor (__init__) of the Block class, which initializes the object. The super().__init__() call initializes the base class (nn.Module).\n",
    "        head_size = n_embd // n_head #This line calculates the size of each attention head (head_size) by dividing the embedding dimension (n_embd) by the number of attention heads (n_head).\n",
    "        self.sa = MultiHeadAttention(n_head, head_size) #This line initializes a multi-head attention mechanism (self.sa) with n_head attention heads, each of size head_size.\n",
    "        self.ffwd = FeedForward(n_embd) #This line initializes a feedforward neural network (self.ffwd) with the specified embedding dimension (n_embd).\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd) #These lines initialize two layer normalization layers (self.ln1 and self.ln2), each normalizing the input to have zero mean and unit variance across the embedding dimension (n_embd).\n",
    "\n",
    "    def forward(self, x): #This line defines the forward pass method (forward) of the Block class, which takes an input tensor x.\n",
    "        y = self.sa(x) #This line passes the input tensor x through the multi-head attention mechanism (self.sa) and assigns the output to y.\n",
    "        x = self.ln1(x + y) #This line adds the output of the self-attention mechanism (y) to the original input tensor (x) and normalizes the result using the first layer normalization layer (self.ln1). This is known as the \"add and norm\" step.\n",
    "        y = self.ffwd(x) #This line passes the normalized tensor x through the feedforward neural network (self.ffwd) and assigns the output to y.\n",
    "        x = self.ln2(x + y) #This line adds the output of the feedforward network (y) to the tensor x from the previous \"add and norm\" step and normalizes the result using the second layer normalization layer (self.ln2). This is another \"add and norm\" step.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d5c85-3fbd-4092-9548-5f2f6fb2ac8a",
   "metadata": {},
   "source": [
    "# The Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38300e78-8655-495e-aa7a-2f5f7ff32f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model parameters...\n",
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #This line initializes an embedding layer (self.token_embedding_table) that converts token indices into dense vectors of size n_embd. The vocabulary size is specified by vocab_size.\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #This line initializes an embedding layer (self.position_embedding_table) that converts position indices into dense vectors of size n_embd. The maximum sequence length is specified by block_size.\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) #This line initializes a sequential container (self.blocks) containing n_layer instances of the Block class. Each block performs self-attention and feedforward operations.\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #This line initializes a layer normalization layer (self.ln_f) that normalizes the final output across the embedding dimension (n_embd).\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)#This line initializes a linear layer (self.lm_head) that projects the final output back to the vocabulary size, producing logits for each token in the vocabulary.\n",
    "\n",
    "        self.apply(self._init_weights) #This line applies the _init_weights method to initialize the weights of the model.\n",
    "\n",
    "    def _init_weights(self, module): \n",
    "        if isinstance(module, nn.Linear): #This method initializes the weights of the model.\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02) #If the module is a linear layer, it initializes the weights with a normal distribution (mean 0, standard deviation 0.02) and sets the biases to zero.\n",
    "            if module.bias is not None: #If the module is an embedding layer, it also initializes the weights with a normal distribution.\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, index, targets=None): #This line defines the forward pass method (forward) of the GPTLanguageModel class, which takes input indices (index) and optional target indices (targets).\n",
    "        B, T = index.shape #This line unpacks the shape of the input tensor index into B (batch size) and T (sequence length).\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index) #This line converts the input token indices (index) into dense vectors using the token embedding table (self.token_embedding_table).\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #This line generates position indices (torch.arange(T, device=device)) and converts them into dense vectors using the position embedding table (self.position_embedding_table).\n",
    "        x = tok_emb + pos_emb #This line adds the token embeddings and positional embeddings to form the input to the neural network.\n",
    "        x = self.blocks(x) #This line passes the combined embeddings through the transformer blocks (self.blocks).\n",
    "        x = self.ln_f(x) #This line normalizes the output of the transformer blocks using the final layer normalization layer (self.ln_f).\n",
    "        logits = self.lm_head(x) #This line projects the normalized output to the vocabulary size using the linear layer (self.lm_head), producing logits for each token.\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #If targets are provided, this block of code reshapes the logits and targets to have a shape of (B*T, C) and (B*T) respectively.\n",
    "            logits = logits.view(B*T, C) #It then computes the cross-entropy loss between the logits and targets.\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens): #This line defines the generate method, which generates new tokens. \n",
    "            index_cond = index[:, -block_size:] #It starts by extracting the last block_size tokens from the input index.\n",
    "            logits, loss = self.forward(index_cond) #This line passes the extracted tokens through the model to get the logits\n",
    "            logits = logits[:, -1, :] #This line extracts the logits corresponding to the last position.\n",
    "            probs = F.softmax(logits, dim = -1) #This line applies the softmax function to the logits to obtain probabilities for each token in the vocabulary.\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #This line samples the next token from the probability distribution using multinomial sampling.\n",
    "            index = torch.cat((index, index_next), dim=1) #This line appends the sampled token to the input index.\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size) #This line initializes an instance of the GPTLanguageModel class with the specified vocabulary size (vocab_size).\n",
    "print('loading model parameters...')\n",
    "with open('model-01.pkl', 'rb') as f: \n",
    "    model = pickle.load(f) #This block of code opens the file model-01.pkl in read-binary mode ('rb') and loads the model parameters from the file using pickle.load.\n",
    "print('model loaded successfully')\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b984d-3dff-4892-a7f8-3f258d3b8e72",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "468bd38f-3470-4878-8255-f611bfdd07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #This decorator disables gradient calculation. It is used to reduce memory consumption and speed up computations during inference, since gradients are not needed.\n",
    "def estimate_loss():\n",
    "    out = {} #This line initializes an empty dictionary out to store the average losses for the training and validation splits.\n",
    "    model.eval() #This line sets the model to evaluation mode using model.eval(). This mode is important because it disables dropout and other training-specific behaviors.\n",
    "    for split in ['train', 'val']: #This line starts a loop that iterates over the two splits: 'train' and 'val'. This loop will estimate the loss for both the training and validation datasets.\n",
    "        losses = torch.zeros(eval_iters) #This line initializes a tensor losses with zeros, having a length equal to eval_iters. This tensor will store the loss values for each iteration.\n",
    "        for k in range(eval_iters): #This nested loop runs for eval_iters iterations. In each iteration:\n",
    "            X, Y = get_batch(split) #Retrieves a batch of input data (X) and target data (Y) for the given split.\n",
    "            logits, loss = model(X, Y) #Computes the logits and loss for the batch using the model.\n",
    "            losses[k] = loss.item() #Stores the computed loss in the losses tensor at index k.\n",
    "        out[split] = losses.mean() #This line calculates the mean of the losses tensor for the current split and stores it in the out dictionary with the split name as the key.\n",
    "    model.train() #This line sets the model back to training mode using model.train(). This mode re-enables dropout and other training-specific behaviors.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4df81f9-742d-4eb9-aa1e-5d9573dfec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0, train loss:2.265, val loss:2.506\n",
      "step:1, train loss:2.280, val loss:2.520\n",
      "step:2, train loss:2.279, val loss:2.537\n",
      "step:3, train loss:2.283, val loss:2.534\n",
      "step:8, train loss:2.277, val loss:2.572\n",
      "step:9, train loss:2.270, val loss:2.514\n",
      "step:10, train loss:2.278, val loss:2.528\n",
      "step:11, train loss:2.275, val loss:2.525\n",
      "step:16, train loss:2.273, val loss:2.584\n",
      "step:17, train loss:2.278, val loss:2.587\n",
      "step:18, train loss:2.272, val loss:2.533\n",
      "step:19, train loss:2.278, val loss:2.523\n",
      "step:24, train loss:2.275, val loss:2.542\n",
      "step:25, train loss:2.274, val loss:2.483\n",
      "step:26, train loss:2.277, val loss:2.551\n",
      "step:27, train loss:2.269, val loss:2.542\n",
      "step:128, train loss:2.263, val loss:2.495\n",
      "step:129, train loss:2.259, val loss:2.534\n",
      "step:130, train loss:2.249, val loss:2.557\n",
      "step:131, train loss:2.257, val loss:2.500\n",
      "step:136, train loss:2.265, val loss:2.535\n",
      "step:137, train loss:2.248, val loss:2.489\n",
      "step:138, train loss:2.261, val loss:2.556\n",
      "step:139, train loss:2.262, val loss:2.516\n",
      "step:144, train loss:2.259, val loss:2.511\n",
      "step:145, train loss:2.253, val loss:2.546\n",
      "step:146, train loss:2.255, val loss:2.477\n",
      "step:147, train loss:2.249, val loss:2.558\n",
      "step:152, train loss:2.249, val loss:2.518\n",
      "step:153, train loss:2.247, val loss:2.532\n",
      "step:154, train loss:2.256, val loss:2.551\n",
      "step:155, train loss:2.247, val loss:2.514\n",
      "2.2998156547546387\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #This line initializes an AdamW optimizer with the parameters of the model (model.parameters()) and a specified learning rate (learning_rate). AdamW is a variant of the Adam optimizer with weight decay regularization.\n",
    "\n",
    "for iter in range(max_iters): #This line starts a loop that iterates from 0 to max_iters-1, where max_iters is the total number of iterations for training.\n",
    "    if iter & eval_iters == 0: #Inside the loop, this block of code checks if the current iteration (iter) is a multiple of eval_iters. \n",
    "        losses = estimate_loss() #If true, it estimates the training and validation losses using the estimate_loss function and prints the current iteration step along with the losses.\n",
    "        print(f\"step:{iter}, train loss:{losses['train']:.3f}, val loss:{losses['val']:.3f}\")\n",
    "\n",
    "    xb, yb = get_batch('train') #This line retrieves a batch of input data (xb) and target data (yb) for training using the get_batch function.\n",
    "\n",
    "    logits, loss = model.forward(xb, yb) #This line performs a forward pass through the model with the training batch (xb, yb) and computes the logits and loss.\n",
    "    optimizer.zero_grad(set_to_none=True) #This line clears the gradients of all optimized parameters by setting them to None. This is a more memory-efficient way to zero the gradients compared to setting them to zero.\n",
    "    loss.backward() #This line computes the gradient of the loss with respect to the model parameters using backpropagation.\n",
    "    optimizer.step() #This line updates the model parameters based on the computed gradients using the optimizer.\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01.pkl', 'wb') as f: #This block of code opens a file named 'model-01.pkl' in write-binary mode ('wb') and saves the model parameters to the file using pickle.dump.\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c97bfb9-4822-4e45-9dba-d8e796b154e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   s ead th.\n",
      "\n",
      "\n",
      "\n",
      "HE Ijany fito -Coighd. Thas cout ng oy Gutedve an. anond Ost The louvernt f t tinis f wn'ro t\n",
      "hey arekyleashiow lander w; an'p are Dofonkin shom s roftan f sar ame debupper\n",
      "fokeif lsind, yckloitidrded b.\n",
      "\n",
      "\n",
      "Shalit tt r l ashed averronolloulylly my, ck the grside satheeyofe ingge t It angy wef liled jo w, ain tailkn th s\n",
      "ird aifel.\n",
      "\n",
      "\n",
      "an t?\"Theanl yy,\"1x dunosithemmas arinceath wnoray 1 I Wic?\" waly at wed cis I blk t th s\n",
      "\n",
      "forournt s 'sipean wo s areage mend prckime tstangl chitemf \n"
     ]
    }
   ],
   "source": [
    "prompt = \" \"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e19482-9a11-4dd4-9216-05092a8544de",
   "metadata": {},
   "source": [
    "# A Chat Bot Loop\n",
    "So I've trained a (rather underwhelming!!) chatbot. \n",
    "\n",
    "This is example code of how to interact with it. \n",
    "\n",
    "I'm not going to make a .py file for it to execute because it isn't worth it, but to show how to integrate. \n",
    "\n",
    "Enjoy this capability in Jupyter notebook for now. Otherwise, visit: <<https://chatgpt.com/>> for a more robust experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "389dd9ae-3809-498a-a3b4-79da87179eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete:\n",
      "hiy the tarms, beve meboul sall yonet as\n",
      "th thesigookad she prkin at tas id fas habod wen of thy\n",
      "arund aro ond Hepermomos lve f hepetat wamofss bepero tpsclla herd tomalis gaurlaly the, t\n",
      "wandck is wis \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " want\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete:\n",
      "want oo dins ho sacethers\n",
      "chrd cnore wat bakes simalat amit poy be\n",
      "paanovesther Jis t bul.\n",
      "\n",
      "Thomow ane ferd tithe s litleris, as, s n, a dothangy cedonly, arvy.\n",
      "\n",
      "\n",
      "\"Wicain \"f ane,\" wed hthellothevimere tir\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "prompt (type 'exit' to quit):\n",
      " exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = input(\"prompt (type 'exit' to quit):\\n\")\n",
    "    if prompt.lower() == 'exit':\n",
    "        print(\"Shutting down...\")\n",
    "        break\n",
    "\n",
    "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    generated_tokens = m.generate(context, max_new_tokens=200)[0].tolist()\n",
    "    generated_chars = decode(generated_tokens)\n",
    "    print(f'complete:\\n{generated_chars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eaf0a-9fa2-4024-b9dd-e0ead8101ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70307fd-a3ae-4ee9-bcff-b3ac6bfbe093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21118563-e2ae-4a65-8a00-e024680e2cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa8b31-7d14-4c93-bdfa-8daad686c644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684830e3-a980-4007-8f6d-0c8bf2e178c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf05f6-50e0-4863-b13e-4035b15e0ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78b42c-8e50-46b5-af3f-6ab92b889011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf5c33-48dd-4ece-87b9-bcea14d671e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
